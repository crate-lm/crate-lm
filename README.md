<!-- <p align="center">
    <img src="./assets/digirl-logo-text.png" alt="logo" width="20%">
</p>
-->


<h3 align="center">
Improving Neuron-level Interpretability with White-box Language Models 
<br>
<b>To Appear at CPAL 2025, Oral</b>

</h3>


<p align="center">
| <a href="https://crate-lm.github.io/"><b>Website</b></a> | <a href="https://arxiv.org/abs/2410.16443"><b>Paper</b></a> | <a href="https://github.com/crate-lm/crate-lm"><b>Checkpoint</b></a> |
</p>

---

Research Code for preprint "Improving Neuron-level Interpretability with White-box Language Models".

[Hao Bai*](https://jackgethome.com), [Yi Ma](https://people.eecs.berkeley.edu/~yima/)<br>
UC Berkeley, UIUC, HKU
<br>
*Work done at UC Berkeley

Content already released:
- model code

Content to be released during **March 2025** (sorry for the long delay, but the PhD application really took me a long while):

- Pre-training script
- Model checkpoints
- Performance evaluation script
- Interpretability evaluation script
